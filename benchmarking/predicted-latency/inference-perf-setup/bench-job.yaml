apiVersion: v1
kind: ConfigMap
metadata:
  name: inference-perf-config-${SUFFIX}
  namespace: default
data:
  config.yml: |
    load:
      type: poisson
      num_workers: 6
      worker_max_concurrency: 1000
      
      stages:
          - rate: 1
            duration: 100
          - rate: 3
            duration: 100
          - rate: 5
            duration: 100
          - rate: 7
            duration: 100
          - rate: 9
            duration: 100
          - rate: 11
            duration: 100
          - rate: 13
            duration: 100
          - rate: 15
            duration: 100


    api: 
      type: completion
      streaming: true 
      headers:         
       x-slo-itl-ms: "${SLO_ITL_MS}"
       x-slo-ttft-ms: "${SLO_TTFT_MS}"
    server:
      type: vllm
      model_name: Qwen/Qwen3-32B
      base_url: ${BASE_URL}   
      ignore_eos: true
    tokenizer:
      pretrained_model_name_or_path: Qwen/Qwen3-32B
    data:
      type: shared_prefix
      shared_prefix:
        num_groups: 1000
        num_prompts_per_group: 6

        system_prompt_len: 1000
        question_len: 30
        question_len_std: 9
        question_len_min: 10
        question_len_max: 10000
        
        output_len: 1000
        output_len_std: 300
        output_len_min: 10
        output_len_max: 10000

        enable_multi_turn_chat: true
    metrics:
      type: prometheus
      prometheus:
        scrape_interval: 15
        google_managed: true
    report:
      request_lifecycle:
        summary: true
        per_stage: true
      prometheus:
        summary: true
        per_stage: true
    storage:
      google_cloud_storage:
        bucket_name: "kaushikmitra-llm-ig-benchmark"
        path: "${OUTPUT_DIR}" 
        report_file_prefix: "${REPORT_PREFIX}"
---
apiVersion: batch/v1
kind: Job
metadata:
  name: inference-perf-${SUFFIX}
spec:
  backoffLimit: 0
  template:
    spec:
      serviceAccountName: "gmp-test-sa"  
      nodeSelector:
        cloud.google.com/gke-nodepool: "pool-4"  
      containers:
        - name: inference-perf
          image: us-docker.pkg.dev/kaushikmitra-gke-dev/kaushikmitra-docker-repo/inference_perf_multi:latest #created from https://github.com/kubernetes-sigs/inference-perf/pull/301
          command: ["inference-perf"]
          args: ["--config_file", "/etc/config/config.yml"]
          env:
            - name: HF_TOKEN
              valueFrom:
                secretKeyRef:
                  name: hf-token
                  key: token
                  optional: true
          volumeMounts:
            - name: config-volume
              mountPath: /etc/config
              readOnly: true
      restartPolicy: Never
      volumes:
        - name: config-volume
          configMap:
            name: inference-perf-config-${SUFFIX}